{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDF files in the directory: 85\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "FILES = ['../pdf/' + fname for fname in os.listdir('../pdf/')]\n",
    "print('Total PDF files in the directory:', len(FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunk from Tips+Tricks_Stunning+Flash+Portraits.pdf: 6\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.partition.strategies import PartitionStrategy\n",
    "\n",
    "EXAMPLE_FILE = FILES[1]\n",
    "example_chunks = partition_pdf(\n",
    "    filename=EXAMPLE_FILE,\n",
    "    strategy=PartitionStrategy.HI_RES,\n",
    "    languages=['eng'],\n",
    "    chunking_strategy='by_title',\n",
    "    max_characters=10000,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    new_after_n_chars=6000,\n",
    "    extract_images_in_pdf=False, # skip image extraction\n",
    "    infer_table_structure=False # skip table extraction\n",
    ")\n",
    "\n",
    "print(f'Total chunk from {EXAMPLE_FILE.split(\"/\")[-1]}: {len(example_chunks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../pdf/Tips+Tricks_Stunning+Flash+Portraits.pdf file chunks:\n",
      "photzyTM\n",
      "\n",
      "TIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH\n",
      "\n",
      "Quick Guide Written by Kevin Landwer-Johan\n",
      "\n",
      "Advertise with us SPONSORED Before you dive into this guide, here's a few other free resources to help you learn photography: Free Photography eBooks 3 Free Photography Cheat Sheets What is Your #1 Photography Killer? Take this 30 second quiz to find out Free access to our library of 250+ Grab 3 free photography cheat the #1 thing holding your downloadable (pdf) tutorials on sheets that will help you photography back. everything you can imagine. understand the basics. Take Quiz → Download Cheat Sheets → Download eBooks → Want quick photography tips? Check out our friends at DailyPhotoTips.com they'll send you 1 solid photography tip to your inbox, 5 days a week. So you can start your day right, with actionable tips to help you on your creative journey. Subscribe now → (free for a limited time) SPONSORED Advertise with us\n",
      "\n",
      "What is Your #1 Photography Killer?\n",
      "\n",
      "Take Quiz →\n",
      "\n",
      "Download eBooks →\n",
      "\n",
      "Do you struggle to capture stunning portraits because the lighting is not how you’d prefer it to be? Adding a pop of flash to portraits is a great way to control the lighting and the atmosphere of your portraits.\n",
      "\n",
      "TIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\n",
      "\n",
      "I understand that most photographers who are not yet confident using their flash will find it challenging to use it for portraits. Flash adds another level of complexity that you have to concentrate on. But once you know how to manage your flash, you will surely appreciate the added depth and dynamic that using it adds to a portrait.\n",
      "\n",
      "3\n",
      "\n",
      "THE FIRST STEPS FOR UNDERSTANDING FLASH FOR PORTRAITS\n",
      "\n",
      "You can use flash any time you want to make a portrait. Flash does not have to be used only when it’s dark. You can balance flash and ambient light to add a professional look to your portrait photographs. When the light is very strong and casts hard shadows, use flash to soften them. You can also use flash to add depth when the available light is very flat and dull.\n",
      "\n",
      "I first began using flash for capturing portraits when I worked as a newspaper photographer. Whenever the ambient light was not suitable, I’d add some flash to my subject. I could not return to my editor without a photo. Any excuse that the light was not good enough would not have been tolerated.\n",
      "\n",
      "As a photojournalist, I preferred to carry minimal equipment, so this meant only one simple flash rather than a more comprehensive lighting kit. I later\n",
      "\n",
      "began to carry a more complex lighting kit when I started my own photography business. Learning to make portraits using a single, simple flash helped me to better understand how to work with ambient light and balance it with my flash.\n",
      "\n",
      "Key Lesson: I found that the key to successful flash use for portraits was to control its level compared to the ambient light, adding either a little more power or less power and adjusting my aperture setting to capture the exposure I wanted.\n",
      "\n",
      "Recommended Reading: If you’d like to learn how to use your flash unit for better photography, grab a copy of Photzy’s Electronic Flash - Parts and Practices premium guide.\n",
      "\n",
      "TIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\n",
      "\n",
      "4\n",
      "\n",
      "Photograph by Kevin Landwer-Johan\n",
      "\n",
      "TIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\n",
      "\n",
      "5\n",
      "\n",
      "Photograph by Kevin Landwer-Johan\n",
      "\n",
      "TIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\n"
     ]
    }
   ],
   "source": [
    "print(f'{EXAMPLE_FILE} file chunks:')\n",
    "for chunk in example_chunks:\n",
    "    print(chunk.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "cleaned_text = [clean_text(chunk.text) for chunk in example_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"photzyTM\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH\\nQuick Guide Written by Kevin Landwer-Johan\\nAdvertise with us SPONSORED Before you dive into this guide, here's a few other free resources to help you learn photography: Free Photography eBooks 3 Free Photography Cheat Sheets What is Your #1 Photography Killer? Take this 30 second quiz to find out Free access to our library of 250+ Grab 3 free photography cheat the #1 thing holding your downloadable (pdf) tutorials on sheets that will help you photography back. everything you can imagine. understand the basics. Take Quiz → Download Cheat Sheets → Download eBooks → Want quick photography tips? Check out our friends at DailyPhotoTips.com they'll send you 1 solid photography tip to your inbox, 5 days a week. So you can start your day right, with actionable tips to help you on your creative journey. Subscribe now → (free for a limited time) SPONSORED Advertise with us\\nWhat is Your #1 Photography Killer?\\nTake Quiz →\\nDownload eBooks →\\nDo you struggle to capture stunning portraits because the lighting is not how you’d prefer it to be? Adding a pop of flash to portraits is a great way to control the lighting and the atmosphere of your portraits.\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\nI understand that most photographers who are not yet confident using their flash will find it challenging to use it for portraits. Flash adds another level of complexity that you have to concentrate on. But once you know how to manage your flash, you will surely appreciate the added depth and dynamic that using it adds to a portrait.\\n3\\nTHE FIRST STEPS FOR UNDERSTANDING FLASH FOR PORTRAITS\\nYou can use flash any time you want to make a portrait. Flash does not have to be used only when it’s dark. You can balance flash and ambient light to add a professional look to your portrait photographs. When the light is very strong and casts hard shadows, use flash to soften them. You can also use flash to add depth when the available light is very flat and dull.\\nI first began using flash for capturing portraits when I worked as a newspaper photographer. Whenever the ambient light was not suitable, I’d add some flash to my subject. I could not return to my editor without a photo. Any excuse that the light was not good enough would not have been tolerated.\\nAs a photojournalist, I preferred to carry minimal equipment, so this meant only one simple flash rather than a more comprehensive lighting kit. I later\\nbegan to carry a more complex lighting kit when I started my own photography business. Learning to make portraits using a single, simple flash helped me to better understand how to work with ambient light and balance it with my flash.\\nKey Lesson: I found that the key to successful flash use for portraits was to control its level compared to the ambient light, adding either a little more power or less power and adjusting my aperture setting to capture the exposure I wanted.\\nRecommended Reading: If you’d like to learn how to use your flash unit for better photography, grab a copy of Photzy’s Electronic Flash - Parts and Practices premium guide.\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\n4\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\n5\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\",\n",
       " 'SETTINGS FOR FLASH PHOTOGRAPHY\\nI manage my exposures manually, including my flash output. I find this allows me the highest level of control. Let me walk you through the process I often use when using flash when taking a portrait photograph.\\nI start by positioning my subject with the desired background and lighting. As I do this, I consider the brightness of the background in relation to the amount of light illuminating my subject. I’ll take a spot meter reading from my subject’s face and then another spot reading from the background. Doing this gives me a clear indication of what difference, if any, there is between the light values in those areas of my composition.\\nNext, I set my flash so it will provide sufficient light on my subject. This may be a little over or under the value of the ambient light. I adjust my camera’s exposure setting to match the output of my flash.\\nWhen my flash is set to emit a greater amount of light than the ambient light on the background, that area of the composition will appear darker.\\nSetting my flash so it outputs less light than what I measure from the background results in the subject being darker than the background.\\n6\\naN | < o !\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\nBalancing the flash output with the ambient light in the background results in a balance of even light on both the subject and background.\\n7\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\nKey Lesson: Controlling your flash manually allows you to be more precise. It allows you to have the same amount of illumination on your subject as the background. It also allows you to make your subject lighter or darker than the background.\\n8\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\nFLASH MODIFIERS FOR PORTRAITS\\nUnmodified flash produces hard light for portraits. This can cause problems with dark shadows if your flash output is not well-balanced with the ambient light. Even when it is nicely balanced, it’s often best to use some kind of modifier with your flash to soften the light.\\nThere are many ways to scatter and soften the light from a flash. Here are some that I enjoy using the most.',\n",
       " 'BOUNCE CARD\\nA bounce card is the simplest flash modifier. It was the only flash modifier I used when I worked at the newspaper because a bounce card is small and flat. It’s easy to tuck into the outside pocket of a camera bag.\\nIt is a piece of white card or plastic, usually about 20cm/8in square with a tab on one side. The tab is used to help hold the card in place on the flash. This is often done with rubber bands or Velcro.\\nA bounce card is most effective when used in a room with a white or neutral-colored ceiling. The flash head is angled up towards the ceiling and the card is mounted behind the flash head so it pushes the light toward the subject.\\n9\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\nREFLECTOR\\nFold-out photographer’s reflectors are another handy flash modifier. You can set up a reflector close to your subject so it’s just outside your composition. Angle the flash so it bounces off the reflector and onto your subject. This diffuses and softens the light from the flash.\\nUsing a reflector is much easier if you have someone to hold it at the best angle.\\nUMBRELLA\\nA small photography umbrella is another way to soften the light from your flash. You can use a white umbrella set up so the light from your flash passes through it. Or you can use a silver umbrella and bounce the light off it and onto your subject.\\nAn umbrella is easier to use on a light stand with a bracket designed to hold both your flash and the umbrella.\\nSOFTBOX\\nThe small softbox I have is my favorite flash modifier. It is about 60cm/24in square. I most often use it mounted on a stand and will sometimes hand-hold it, depending on where I am working.\\nI prefer using the softbox because it produces the best quality of light diffusion. It is a little more cumbersome and not so easy to use as other flash modifiers but is great once I have it set up.\\n10\\nos. *,\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM',\n",
       " 'GELS - FOR STYLE AND COLOR CORRECTION\\nWhen the ambient light you balance your flash with is electric, it may introduce a color cast. Some electric lights emit warm or cool light. Others are daylight balanced.\\nDaylight-balanced light bulbs require no color correction filtering. They emit light of the same color temperature as your flash. If the ambient eclectic light is warm, use the correct orange gel on your flash. This will produce light of the same color temperature as the light fitting. The correct blue or green gel balances light from a cool light source.\\nYou can style the light from your flash using different colored gels for creative effect.\\nKey Lesson: When the ambient light is soft, it’s also best to soften the light from your flash. Doing this and balancing the flash output with the ambient light helps to produce a more natural-looking portrait.\\n11\\nON-CAMERA AND OFF-CAMERA FLASH\\nMounting a flash on your camera makes for a convenient way to add more light to your portrait subject. But it’s not going to provide nice natural- looking light.\\nwith the small softbox, allows me to position the light where I want it. Sometimes I’ll hold the flash in my left hand with my arm outstretched if I don’t have a light stand or an assistant.\\nOn camera, flash adds light to a subject as though it’s coming directly from the viewer’s perspective. This is never how we see a person in reality. The light from the flash is likely to cast shadows that look odd because this is not how we naturally see people.\\nUsing off-camera flash also gives you more control over where the shadows fall. If your subject is close to the background, you can often position your flash so that the shadow is minimalized.\\nMoving the flash off the camera and positioning it to one side of your subject helps to produce a more normal appearance.\\nSome camera flash systems allow you to trigger the flash directly from the camera. Other systems require the use of external triggers to operate the flash remotely.\\nI prefer to use off-camera flash when making portraits. Placing my flash on a light stand, along\\nKey Lesson: Managing off-camera flash adds another level of complexity to a portrait session. Once you’ve practiced using this technique, you will discover that the benefits make it worthwhile.\\nRecommended Reading: If you’d like to learn how to use your flash unit for better photography, grab a copy of Photzy’s Electronic Flash - Parts and Practices premium guide.\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\n12\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\n13\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM',\n",
       " \"CONCLUSION\\nAdding flash to portraits often helps you create more interesting and dynamic portraits. When you’ve learned to control the amount and quality of light, your portraits will take on a whole new look.\\nTake your time and practice. Work with a friend or a family member who is willing to sit for you patiently. Experiment with different camera and flash setting combinations to discover what works best for you. Then try adding some kind of diffusion to soften the light.\\nFor some photos, have your subject positioned near the background. For others, move them away from the background so that the light from the flash only affects your subject and not the background.\\nOnce you’ve taken a good number of pictures, study them. Look at the balance between the ambient light and light from your flash. Check the quality of light and compare it between portraits with no flash diffusion and those where you did use a diffuser. Then go back and take some more. The more you practice, the more confident you will become in making stunning portraits using flash.\\n14\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\n15\\nSelf-Check Quiz:\\n1) Can you use flash when there is sufficient ambient light to make a portrait?\\n2) Why is balancing flash output with ambient light a good idea?\\n3) What effect on the background does setting your flash to slightly overexpose create?\\n4) Name three modifiers that soften light from a flash.\\n5) What’s one purpose of using colored gels with flash?\\n6) How can using off-camera flash help improve your portraits?\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\n16\\nAdvertise with us SPONSORED Hey there! Let's get real for a minute... Learning photography can be super challenging! But we're here to help you every step of the way! Here are 3 of our most useful (and FREE!) photography resources: Free Photography eBooks 3 Free Photography Cheat Sheets What is Your #1 Photography Killer? Take this 30 second quiz to find out Free access to our library of 250+ Grab 3 free photography cheat the #1 thing holding your downloadable (pdf) tutorials on sheets that will help you photography back. everything you can imagine. understand the basics. Download eBooks → Take Quiz → Download Cheat Sheets → Want quick photography tips? Check out our friends at DailyPhotoTips.com they'll send you 1 solid photography tip to your inbox, 5 days a week. So you can start your day right, with actionable tips to help you on your creative journey. Subscribe now → (free for a limited time) SPONSORED Advertise with us\\nWhat is Your #1 Photography Killer?\\nDownload eBooks →\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize langchain and chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedding_function = OllamaEmbeddings(model='nomic-embed-text')\n",
    "vectorstore = Chroma(\n",
    "    collection_name='photography_collection', \n",
    "    embedding_function=embedding_function,\n",
    "    persist_directory='../chromadb'\n",
    ")\n",
    "\n",
    "vectorstore.reset_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "docstore = InMemoryStore()\n",
    "id_key = 'doc_id'\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    id_key=id_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "doc_ids = [str(uuid4()) for _ in example_chunks]\n",
    "texts = [\n",
    "    Document(\n",
    "        page_content=text,\n",
    "        metadata={\n",
    "            id_key: doc_ids[i],\n",
    "            'content': text,\n",
    "            'filename': example_chunks[i].metadata.filename,\n",
    "            'page_number': example_chunks[i].metadata.page_number\n",
    "        }\n",
    "    ) for i, text in enumerate(cleaned_text)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, cleaned_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SETTINGS FOR FLASH PHOTOGRAPHY\\nI manage my exposures manually, including my flash output. I find this allows me the highest level of control. Let me walk you through the process I often use when using flash when taking a portrait photograph.\\nI start by positioning my subject with the desired background and lighting. As I do this, I consider the brightness of the background in relation to the amount of light illuminating my subject. I’ll take a spot meter reading from my subject’s face and then another spot reading from the background. Doing this gives me a clear indication of what difference, if any, there is between the light values in those areas of my composition.\\nNext, I set my flash so it will provide sufficient light on my subject. This may be a little over or under the value of the ambient light. I adjust my camera’s exposure setting to match the output of my flash.\\nWhen my flash is set to emit a greater amount of light than the ambient light on the background, that area of the composition will appear darker.\\nSetting my flash so it outputs less light than what I measure from the background results in the subject being darker than the background.\\n6\\naN | < o !\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\nBalancing the flash output with the ambient light in the background results in a balance of even light on both the subject and background.\\n7\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\nKey Lesson: Controlling your flash manually allows you to be more precise. It allows you to have the same amount of illumination on your subject as the background. It also allows you to make your subject lighter or darker than the background.\\n8\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\nFLASH MODIFIERS FOR PORTRAITS\\nUnmodified flash produces hard light for portraits. This can cause problems with dark shadows if your flash output is not well-balanced with the ambient light. Even when it is nicely balanced, it’s often best to use some kind of modifier with your flash to soften the light.\\nThere are many ways to scatter and soften the light from a flash. Here are some that I enjoy using the most.',\n",
       " \"photzyTM\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH\\nQuick Guide Written by Kevin Landwer-Johan\\nAdvertise with us SPONSORED Before you dive into this guide, here's a few other free resources to help you learn photography: Free Photography eBooks 3 Free Photography Cheat Sheets What is Your #1 Photography Killer? Take this 30 second quiz to find out Free access to our library of 250+ Grab 3 free photography cheat the #1 thing holding your downloadable (pdf) tutorials on sheets that will help you photography back. everything you can imagine. understand the basics. Take Quiz → Download Cheat Sheets → Download eBooks → Want quick photography tips? Check out our friends at DailyPhotoTips.com they'll send you 1 solid photography tip to your inbox, 5 days a week. So you can start your day right, with actionable tips to help you on your creative journey. Subscribe now → (free for a limited time) SPONSORED Advertise with us\\nWhat is Your #1 Photography Killer?\\nTake Quiz →\\nDownload eBooks →\\nDo you struggle to capture stunning portraits because the lighting is not how you’d prefer it to be? Adding a pop of flash to portraits is a great way to control the lighting and the atmosphere of your portraits.\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\nI understand that most photographers who are not yet confident using their flash will find it challenging to use it for portraits. Flash adds another level of complexity that you have to concentrate on. But once you know how to manage your flash, you will surely appreciate the added depth and dynamic that using it adds to a portrait.\\n3\\nTHE FIRST STEPS FOR UNDERSTANDING FLASH FOR PORTRAITS\\nYou can use flash any time you want to make a portrait. Flash does not have to be used only when it’s dark. You can balance flash and ambient light to add a professional look to your portrait photographs. When the light is very strong and casts hard shadows, use flash to soften them. You can also use flash to add depth when the available light is very flat and dull.\\nI first began using flash for capturing portraits when I worked as a newspaper photographer. Whenever the ambient light was not suitable, I’d add some flash to my subject. I could not return to my editor without a photo. Any excuse that the light was not good enough would not have been tolerated.\\nAs a photojournalist, I preferred to carry minimal equipment, so this meant only one simple flash rather than a more comprehensive lighting kit. I later\\nbegan to carry a more complex lighting kit when I started my own photography business. Learning to make portraits using a single, simple flash helped me to better understand how to work with ambient light and balance it with my flash.\\nKey Lesson: I found that the key to successful flash use for portraits was to control its level compared to the ambient light, adding either a little more power or less power and adjusting my aperture setting to capture the exposure I wanted.\\nRecommended Reading: If you’d like to learn how to use your flash unit for better photography, grab a copy of Photzy’s Electronic Flash - Parts and Practices premium guide.\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\n4\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\n5\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\",\n",
       " 'BOUNCE CARD\\nA bounce card is the simplest flash modifier. It was the only flash modifier I used when I worked at the newspaper because a bounce card is small and flat. It’s easy to tuck into the outside pocket of a camera bag.\\nIt is a piece of white card or plastic, usually about 20cm/8in square with a tab on one side. The tab is used to help hold the card in place on the flash. This is often done with rubber bands or Velcro.\\nA bounce card is most effective when used in a room with a white or neutral-colored ceiling. The flash head is angled up towards the ceiling and the card is mounted behind the flash head so it pushes the light toward the subject.\\n9\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\nREFLECTOR\\nFold-out photographer’s reflectors are another handy flash modifier. You can set up a reflector close to your subject so it’s just outside your composition. Angle the flash so it bounces off the reflector and onto your subject. This diffuses and softens the light from the flash.\\nUsing a reflector is much easier if you have someone to hold it at the best angle.\\nUMBRELLA\\nA small photography umbrella is another way to soften the light from your flash. You can use a white umbrella set up so the light from your flash passes through it. Or you can use a silver umbrella and bounce the light off it and onto your subject.\\nAn umbrella is easier to use on a light stand with a bracket designed to hold both your flash and the umbrella.\\nSOFTBOX\\nThe small softbox I have is my favorite flash modifier. It is about 60cm/24in square. I most often use it mounted on a stand and will sometimes hand-hold it, depending on where I am working.\\nI prefer using the softbox because it produces the best quality of light diffusion. It is a little more cumbersome and not so easy to use as other flash modifiers but is great once I have it set up.\\n10\\nos. *,\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM',\n",
       " 'GELS - FOR STYLE AND COLOR CORRECTION\\nWhen the ambient light you balance your flash with is electric, it may introduce a color cast. Some electric lights emit warm or cool light. Others are daylight balanced.\\nDaylight-balanced light bulbs require no color correction filtering. They emit light of the same color temperature as your flash. If the ambient eclectic light is warm, use the correct orange gel on your flash. This will produce light of the same color temperature as the light fitting. The correct blue or green gel balances light from a cool light source.\\nYou can style the light from your flash using different colored gels for creative effect.\\nKey Lesson: When the ambient light is soft, it’s also best to soften the light from your flash. Doing this and balancing the flash output with the ambient light helps to produce a more natural-looking portrait.\\n11\\nON-CAMERA AND OFF-CAMERA FLASH\\nMounting a flash on your camera makes for a convenient way to add more light to your portrait subject. But it’s not going to provide nice natural- looking light.\\nwith the small softbox, allows me to position the light where I want it. Sometimes I’ll hold the flash in my left hand with my arm outstretched if I don’t have a light stand or an assistant.\\nOn camera, flash adds light to a subject as though it’s coming directly from the viewer’s perspective. This is never how we see a person in reality. The light from the flash is likely to cast shadows that look odd because this is not how we naturally see people.\\nUsing off-camera flash also gives you more control over where the shadows fall. If your subject is close to the background, you can often position your flash so that the shadow is minimalized.\\nMoving the flash off the camera and positioning it to one side of your subject helps to produce a more normal appearance.\\nSome camera flash systems allow you to trigger the flash directly from the camera. Other systems require the use of external triggers to operate the flash remotely.\\nI prefer to use off-camera flash when making portraits. Placing my flash on a light stand, along\\nKey Lesson: Managing off-camera flash adds another level of complexity to a portrait session. Once you’ve practiced using this technique, you will discover that the benefits make it worthwhile.\\nRecommended Reading: If you’d like to learn how to use your flash unit for better photography, grab a copy of Photzy’s Electronic Flash - Parts and Practices premium guide.\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\n12\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\n13\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = retriever.invoke('How do I control my flash during my photography?')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class Question(BaseModel):\n",
    "    prompt: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Question(prompt=[\"How can one control lighting conditions when taking portraits with their camera's flash?\", 'What methods are recommended for balancing ambient light with the use of a flash in photography?', 'What is an effective way to practice and improve portrait photography using a single flash?'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "model = ChatOllama(model='deepseek-r1', temperature=.8)\n",
    "question_parser = PydanticOutputParser(pydantic_object=Question)\n",
    "question_prompt = PromptTemplate(\n",
    "    template=\"\"\"Generate 3 different FaQ questions based on the text and only generate questions strictly on the text provided. Do not include any markdown formatting or code blocks in your response.\n",
    "    {format_instructions}\n",
    "    {chunk}\n",
    "\n",
    "    Only return the output in the following format:\n",
    "    {{\n",
    "        \"prompt\": [\"First question here\", \"Second question here\", \"Third question here\"]\n",
    "    }}\n",
    "    \"\"\",\n",
    "    input_variables=['chunk'],\n",
    "    partial_variables={\"format_instructions\": question_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "clean_output = RunnableLambda(\n",
    "    lambda x: x.split(\"</think>\")[-1].strip() if isinstance(x, str) \n",
    "    else x.content.split(\"</think>\")[-1].strip()\n",
    ")\n",
    "\n",
    "question_chain = question_prompt | model | clean_output | question_parser\n",
    "example_questions = question_chain.invoke(cleaned_text[0])\n",
    "example_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answer candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Answer(prompt=['Use off-camera flash to position the light where you want shadows to fall.', 'Apply colored gel filters to correct any warm or cool tones from ambient light.', \"Utilize a softbox to diffuse and soften the flash's output for a natural appearance.\"])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    prompt: List[str]\n",
    "\n",
    "def parse_docs(docs):\n",
    "    return [doc for doc in docs]\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "    question = kwargs['question']\n",
    "    context = kwargs['context']\n",
    "    \n",
    "    answer_prompt = [\n",
    "        {\n",
    "            'type': 'text',\n",
    "            'text': f\"\"\"Generate 3 different FaQ answers based only on the following context, which only include text. Do not include any markdown formatting or code blocks in your response.\n",
    "            Context: {context}\n",
    "            Question: {question}\n",
    "\n",
    "            Return the output in the following JSON format:\n",
    "            {{\n",
    "                \"prompt\": [\"First answer here\", \"Second answer here\", \"Third answer here\"]\n",
    "            }}\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            HumanMessage(content=answer_prompt)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "answer_chain = (\n",
    "    {\n",
    "        'context': retriever | RunnableLambda(parse_docs),\n",
    "        'question': RunnablePassthrough()\n",
    "    }\n",
    "    | RunnableLambda(build_prompt)\n",
    "    | model\n",
    "    | clean_output\n",
    "    | PydanticOutputParser(pydantic_object=Answer)\n",
    ")\n",
    "\n",
    "example_question = example_questions.prompt[0]\n",
    "candidate_example_question = answer_chain.invoke(example_question)\n",
    "candidate_example_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Use off-camera flash to position the light where you want shadows to fall.',\n",
       " \"Utilize a softbox to diffuse and soften the flash's output for a natural appearance.\",\n",
       " 'Apply colored gel filters to correct any warm or cool tones from ambient light.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "reference = retriever.vectorstore.similarity_search(example_question, k=1)[0].page_content\n",
    "\n",
    "# score similarity\n",
    "candidate_embeds = embedding_function.embed_documents(candidate_example_question.prompt)\n",
    "reference_embed = embedding_function.embed_query(reference)\n",
    "\n",
    "scores = [\n",
    "    torch.nn.functional.cosine_similarity(\n",
    "        torch.tensor(reference_embed),\n",
    "        torch.tensor(candidate_embed),\n",
    "        dim=0\n",
    "    ).item()\n",
    "    for candidate_embed in candidate_embeds\n",
    "]\n",
    "\n",
    "sorted_pairs = sorted(zip(candidate_example_question.prompt, scores), key=lambda x: x[1], reverse=True)\n",
    "sorted_answer = [pair[0] for pair in sorted_pairs]\n",
    "sorted_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create preference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"How can one control lighting conditions when taking portraits with their camera's flash?\",\n",
       " 'chosen': 'Use off-camera flash to position the light where you want shadows to fall.',\n",
       " 'rejected': 'Apply colored gel filters to correct any warm or cool tones from ambient light.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = {\n",
    "    'prompt': example_question,\n",
    "    'chosen': sorted_answer[0],\n",
    "    'rejected': sorted_answer[-1]\n",
    "}\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "def chunk_file(fname):\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    chunks = partition_pdf(\n",
    "        filename=fname,\n",
    "        strategy=PartitionStrategy.HI_RES,\n",
    "        languages=['eng'],\n",
    "        chunking_strategy='by_title',\n",
    "        max_characters=10000,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        new_after_n_chars=6000,\n",
    "        extract_images_in_pdf=False, # skip image extraction\n",
    "        infer_table_structure=False # skip table extraction\n",
    "    )\n",
    "\n",
    "    return [clean_text(chunk.text) for chunk in chunks]\n",
    "\n",
    "chunk_dict = {}\n",
    "data_dir = os.listdir('../pdf')\n",
    "for i, fname in enumerate(data_dir):\n",
    "    print(f'[{i+1}/{len(data_dir)}] Chunking {fname}')\n",
    "    key = fname.split('.')[0]\n",
    "    path = os.path.join('../pdf', fname)\n",
    "    result = chunk_file(path)\n",
    "    chunk_dict[key] = result\n",
    "    print(f'[{i+1}/{len(data_dir)}] Chunking {fname} - Total chunk: {len(result)}')\n",
    "display.clear_output()\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "vectorstore.reset_collection()\n",
    "\n",
    "def populate_db(fname, chunks):\n",
    "    doc_ids = [str(uuid4()) for _ in chunks]\n",
    "    texts = [\n",
    "        Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                id_key: doc_ids[i],\n",
    "                'content': text,\n",
    "                'filename': fname\n",
    "            }\n",
    "        ) for i, text in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "    retriever.vectorstore.add_documents(texts)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, chunks)))\n",
    "\n",
    "for k, v in chunk_dict.items():\n",
    "    print(f'Popuplating {k}')\n",
    "    populate_db(k, v)\n",
    "display.clear_output()\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SETTINGS FOR FLASH PHOTOGRAPHY\\nI manage my exposures manually, including my flash output. I find this allows me the highest level of control. Let me walk you through the process I often use when using flash when taking a portrait photograph.\\nI start by positioning my subject with the desired background and lighting. As I do this, I consider the brightness of the background in relation to the amount of light illuminating my subject. I’ll take a spot meter reading from my subject’s face and then another spot reading from the background. Doing this gives me a clear indication of what difference, if any, there is between the light values in those areas of my composition.\\nNext, I set my flash so it will provide sufficient light on my subject. This may be a little over or under the value of the ambient light. I adjust my camera’s exposure setting to match the output of my flash.\\nWhen my flash is set to emit a greater amount of light than the ambient light on the background, that area of the composition will appear darker.\\nSetting my flash so it outputs less light than what I measure from the background results in the subject being darker than the background.\\n6\\naN | < o !\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\nBalancing the flash output with the ambient light in the background results in a balance of even light on both the subject and background.\\n7\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\nKey Lesson: Controlling your flash manually allows you to be more precise. It allows you to have the same amount of illumination on your subject as the background. It also allows you to make your subject lighter or darker than the background.\\n8\\nPhotograph by Kevin Landwer-Johan\\nTIPS AND TRICKS FOR CAPTURING STUNNING PORTRAITS USING YOUR FLASH // © PHOTZY.COM\\nFLASH MODIFIERS FOR PORTRAITS\\nUnmodified flash produces hard light for portraits. This can cause problems with dark shadows if your flash output is not well-balanced with the ambient light. Even when it is nicely balanced, it’s often best to use some kind of modifier with your flash to soften the light.\\nThere are many ways to scatter and soften the light from a flash. Here are some that I enjoy using the most.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = retriever.invoke('How do I control my flash during my photography?')\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating question for Camera+Insurance_WorthIt\n",
      "Generating question for Tips+Tricks_Stunning+Flash+Portraits\n",
      "Generating question for How+to+Optimize+Motion+in+Your+Photos\n",
      "Generating question for Vertical+Landscape\n",
      "Generating question for Pros+and+Cons+Different+Sensor+Sizes\n",
      "Generating question for Camera+Basics_Drive+Modes+Explained\n",
      "Generating question for Morning+Light\n",
      "Generating question for Get+and+Stay+Inspired\n",
      "Generating question for Color+Theory_Make+it+Work\n",
      "Generating question for Wide+Angle+Photography+w+Purpose\n",
      "Generating question for Sense+of+Depth+in+Portraits\n",
      "Generating question for Shooting+Through+Objects\n",
      "Generating question for Out+the+Backdoor\n",
      "Generating question for What+to+Do_Low+Light\n",
      "Generating question for PostProcessing+Treatments\n",
      "Generating question for Mixed+Media\n",
      "Generating question for How+to+Approach+a+Stranger\n",
      "Generating question for The+Camera+In+Your+Hand\n",
      "Generating question for Props_Using+Objects\n",
      "Generating question for Highest+Paid+Art+Photograhpers\n",
      "Generating question for DOF_How+to+control+it\n",
      "Generating question for vision\n",
      "Generating question for Tripods+The+Same\n",
      "Generating question for Primary+Colors\n",
      "Generating question for What+is+Light+Painting\n",
      "Generating question for Fill+Flash\n",
      "Generating question for What+Makes+A+Good+Photo\n",
      "Generating question for Finding+Beauty+Everywhere\n",
      "Generating question for Viewfinder+vs+LCD\n",
      "Generating question for Seven+Photo+Composition+Skills\n",
      "Generating question for Getting+Started+Documentary+Photography\n",
      "Generating question for Composition+Framing+and+Space\n",
      "Generating question for Polaroid+History\n",
      "Generating question for Street-Photography-an-eBook-by-Alex-Coghe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating question for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[0;32m---> 29\u001b[0m         question \u001b[38;5;241m=\u001b[39m generate_question(chunk)\n\u001b[1;32m     30\u001b[0m         question_lists\u001b[38;5;241m.\u001b[39mappend(question)\n\u001b[1;32m     31\u001b[0m display\u001b[38;5;241m.\u001b[39mclear_output()\n",
      "Cell \u001b[0;32mIn[18], line 23\u001b[0m, in \u001b[0;36mgenerate_question\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     17\u001b[0m clean_output \u001b[38;5;241m=\u001b[39m RunnableLambda(\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</think>\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</think>\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m model \u001b[38;5;241m|\u001b[39m clean_output \u001b[38;5;241m|\u001b[39m parser\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chain\u001b[38;5;241m.\u001b[39minvoke(text)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3021\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    634\u001b[0m                 m,\n\u001b[1;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    638\u001b[0m             )\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    852\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    853\u001b[0m         )\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_ollama/chat_models.py:701\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    696\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    700\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 701\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_stream_with_aggregation(\n\u001b[1;32m    702\u001b[0m         messages, stop, run_manager, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    703\u001b[0m     )\n\u001b[1;32m    704\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    705\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    706\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    707\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    711\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    712\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_ollama/chat_models.py:602\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    595\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    600\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    601\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    604\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m ChatGenerationChunk(\n\u001b[1;32m    605\u001b[0m                 message\u001b[38;5;241m=\u001b[39mAIMessageChunk(\n\u001b[1;32m    606\u001b[0m                     content\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 ),\n\u001b[1;32m    620\u001b[0m             )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_ollama/chat_models.py:589\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m chat_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_params(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 589\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/ollama/_client.py:169\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    167\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[1;32m    170\u001b[0m   part \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[1;32m    171\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m:=\u001b[39m part\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/httpx/_models.py:863\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    861\u001b[0m decoder \u001b[38;5;241m=\u001b[39m LineDecoder()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_text():\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m decoder\u001b[38;5;241m.\u001b[39mdecode(text):\n\u001b[1;32m    865\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m line\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/httpx/_models.py:850\u001b[0m, in \u001b[0;36mResponse.iter_text\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    848\u001b[0m chunker \u001b[38;5;241m=\u001b[39m TextChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m byte_content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_bytes():\n\u001b[1;32m    851\u001b[0m         text_content \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(byte_content)\n\u001b[1;32m    852\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(text_content):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/httpx/_models.py:831\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    829\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 831\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_raw():\n\u001b[1;32m    832\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/httpx/_models.py:885\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    882\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 885\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m    886\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/httpx/_client.py:127\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/httpx/_transports/default.py:116\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[1;32m    117\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:407\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:403\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    404\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/httpcore/_sync/http11.py:342\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/httpcore/_sync/http11.py:334\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/httpcore/_sync/http11.py:203\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    200\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generate_question(text):\n",
    "    parser = PydanticOutputParser(pydantic_object=Question)\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Generate 3 different FaQ question based on the text and only generate question strictly on the text provided. Do not include any markdown formatting or code blocks in your response.\n",
    "        {format_instructions}\n",
    "        {chunk}\n",
    "\n",
    "        Return the output in the following format:\n",
    "        {{\n",
    "            \"prompt\": [\"First question here\", \"Second question here\", \"Third question here\"]\n",
    "        }}\n",
    "        \"\"\",\n",
    "        input_variables=['chunk'],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    clean_output = RunnableLambda(\n",
    "        lambda x: x.split(\"</think>\")[-1].strip() if isinstance(x, str) \n",
    "        else x.content.split(\"</think>\")[-1].strip()\n",
    "    )\n",
    "\n",
    "    chain = prompt | model | clean_output | parser\n",
    "    return chain.invoke(text)\n",
    "\n",
    "question_lists = []\n",
    "for k, chunks in chunk_dict.items():\n",
    "    print(f'Generating question for {k}')\n",
    "    for chunk in chunks:\n",
    "        question = generate_question(chunk)\n",
    "        question_lists.append(question)\n",
    "display.clear_output()\n",
    "question_lists[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m question_lists:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mprompt:\n\u001b[0;32m---> 45\u001b[0m         qna_pairs[question] \u001b[38;5;241m=\u001b[39m generate_answer(question)\n",
      "Cell \u001b[0;32mIn[36], line 40\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m     25\u001b[0m         [\n\u001b[1;32m     26\u001b[0m             HumanMessage(content\u001b[38;5;241m=\u001b[39manswer_prompt)\n\u001b[1;32m     27\u001b[0m         ]\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     30\u001b[0m chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     31\u001b[0m     {\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: retriever \u001b[38;5;241m|\u001b[39m RunnableLambda(parse_docs),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;241m|\u001b[39m PydanticOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39mAnswer)\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chain\u001b[38;5;241m.\u001b[39minvoke(text)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3021\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    634\u001b[0m                 m,\n\u001b[1;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    638\u001b[0m             )\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    852\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    853\u001b[0m         )\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_anthropic/chat_models.py:797\u001b[0m, in \u001b[0;36mChatAnthropic._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[1;32m    796\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_payload(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 797\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/anthropic/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/anthropic/resources/messages/messages.py:901\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[1;32m    895\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    896\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    898\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    899\u001b[0m     )\n\u001b[0;32m--> 901\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/v1/messages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    903\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    904\u001b[0m         {\n\u001b[1;32m    905\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    906\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    907\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    908\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    909\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop_sequences,\n\u001b[1;32m    910\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    911\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m: system,\n\u001b[1;32m    912\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    914\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    915\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_k,\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    917\u001b[0m         },\n\u001b[1;32m    918\u001b[0m         message_create_params\u001b[38;5;241m.\u001b[39mMessageCreateParams,\n\u001b[1;32m    919\u001b[0m     ),\n\u001b[1;32m    920\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    921\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    922\u001b[0m     ),\n\u001b[1;32m    923\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mMessage,\n\u001b[1;32m    924\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    925\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mStream[RawMessageStreamEvent],\n\u001b[1;32m    926\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/anthropic/_base_client.py:1279\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1267\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1275\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1276\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1277\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1278\u001b[0m     )\n\u001b[0;32m-> 1279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/anthropic/_base_client.py:956\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    957\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    958\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    959\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    960\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    961\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m    962\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/anthropic/_base_client.py:1060\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1059\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1063\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1064\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1069\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
     ]
    }
   ],
   "source": [
    "def generate_answer(text):\n",
    "    def parse_docs(docs):\n",
    "        return [doc for doc in docs]\n",
    "\n",
    "    def build_prompt(kwargs):\n",
    "        question = kwargs['question']\n",
    "        context = kwargs['context']\n",
    "        \n",
    "        answer_prompt = [\n",
    "            {\n",
    "                'type': 'text',\n",
    "                'text': f\"\"\"Generate 3 different FaQ answers based only on the following context, which only include text. Do not include any markdown formatting or code blocks in your response.\n",
    "                Context: {context}\n",
    "                Question: {question}\n",
    "\n",
    "                Return the output in the following JSON format:\n",
    "                {{\n",
    "                    \"prompt\": [\"First answer here\", \"Second answer here\", \"Third answer here\"]\n",
    "                }}\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        return ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                HumanMessage(content=answer_prompt)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            'context': retriever | RunnableLambda(parse_docs),\n",
    "            'question': RunnablePassthrough()\n",
    "        }\n",
    "        | RunnableLambda(build_prompt)\n",
    "        | model\n",
    "        | clean_output\n",
    "        | PydanticOutputParser(pydantic_object=Answer)\n",
    "    )\n",
    "\n",
    "    return chain.invoke(text)\n",
    "\n",
    "qna_pairs = {}\n",
    "for chunk in question_lists:\n",
    "    print(f'Generating answer for {chunk.prompt}')\n",
    "    for question in chunk.prompt:\n",
    "        qna_pairs[question] = generate_answer(question)\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
