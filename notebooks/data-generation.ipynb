{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDF files in the directory: 19\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "FILES = ['../pdf/' + fname for fname in os.listdir('../pdf/')]\n",
    "print('Total PDF files in the directory:', len(FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunk from Color+Theory_Make+it+Work.pdf: 6\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.partition.strategies import PartitionStrategy\n",
    "\n",
    "EXAMPLE_FILE = FILES[1]\n",
    "example_chunks = partition_pdf(\n",
    "    filename=EXAMPLE_FILE,\n",
    "    strategy=PartitionStrategy.HI_RES,\n",
    "    languages=['eng'],\n",
    "    chunking_strategy='by_title',\n",
    "    max_characters=10000,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    new_after_n_chars=6000,\n",
    "    extract_images_in_pdf=False, # skip image extraction\n",
    "    infer_table_structure=False # skip table extraction\n",
    ")\n",
    "\n",
    "print(f'Total chunk from {EXAMPLE_FILE.split(\"/\")[-1]}: {len(example_chunks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../pdf/Color+Theory_Make+it+Work.pdf file chunks:\n",
      "photzyTM\n",
      "\n",
      "COLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU)\n",
      "\n",
      "Quick Guide Written by Jo Plumridge\n",
      "\n",
      "Advertise with us SPONSORED Before you dive into this guide, here's a few other free resources to help you learn photography: Free Photography eBooks 3 Free Photography Cheat Sheets What is Your #1 Photography Killer? Take this 30 second quiz to find out Free access to our library of 250+ Grab 3 free photography cheat the #1 thing holding your downloadable (pdf) tutorials on sheets that will help you photography back. everything you can imagine. understand the basics. Take Quiz → Download Cheat Sheets → Download eBooks → Want quick photography tips? Check out our friends at DailyPhotoTips.com they'll send you 1 solid photography tip to your inbox, 5 days a week. So you can start your day right, with actionable tips to help you on your creative journey. Subscribe now → (free for a limited time) SPONSORED Advertise with us\n",
      "\n",
      "What is Your #1 Photography Killer?\n",
      "\n",
      "Take Quiz →\n",
      "\n",
      "Download eBooks →\n",
      "\n",
      "Color is everywhere in photography (unless, of course, you’re shooting in black and white), and understanding how to use it correctly is an important step in your photographic journey. Color theory is something that every art student will be familiar with, but it seems to sometimes be a little overlooked in photography. So in this guide I’ll be explaining color theory to you and looking in detail at the color wheel. Color can be such a useful tool for evoking emotion, so it’s worth understanding how to utilize it.\n",
      "\n",
      "COLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\n",
      "\n",
      "Here’s what we’ll cover:\n",
      "\n",
      "· The color wheel\n",
      "\n",
      "· Emotions\n",
      "\n",
      "· Color schemes\n",
      "\n",
      "· Color variables\n",
      "\n",
      "· Tips for experimenting\n",
      "\n",
      "· A quick list of helpful tools\n",
      "\n",
      "sa\n",
      "\n",
      "Recommended Reading: Want to create\n",
      "\n",
      "memorable, fascinating, and impressive color photographs? Grab a copy of Photzy’s premium guide: Rich and Vibrant Color Photography Volume 1.\n",
      "\n",
      "3\n",
      "\n",
      "primary complementary complementary complementary\n",
      "\n",
      "The Color Wheel\n",
      "\n",
      "COLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\n"
     ]
    }
   ],
   "source": [
    "print(f'{EXAMPLE_FILE} file chunks:')\n",
    "for chunk in example_chunks:\n",
    "    print(chunk.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "cleaned_text = [clean_text(chunk.text) for chunk in example_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"photzyTM\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU)\\nQuick Guide Written by Jo Plumridge\\nAdvertise with us SPONSORED Before you dive into this guide, here's a few other free resources to help you learn photography: Free Photography eBooks 3 Free Photography Cheat Sheets What is Your #1 Photography Killer? Take this 30 second quiz to find out Free access to our library of 250+ Grab 3 free photography cheat the #1 thing holding your downloadable (pdf) tutorials on sheets that will help you photography back. everything you can imagine. understand the basics. Take Quiz → Download Cheat Sheets → Download eBooks → Want quick photography tips? Check out our friends at DailyPhotoTips.com they'll send you 1 solid photography tip to your inbox, 5 days a week. So you can start your day right, with actionable tips to help you on your creative journey. Subscribe now → (free for a limited time) SPONSORED Advertise with us\\nWhat is Your #1 Photography Killer?\\nTake Quiz →\\nDownload eBooks →\\nColor is everywhere in photography (unless, of course, you’re shooting in black and white), and understanding how to use it correctly is an important step in your photographic journey. Color theory is something that every art student will be familiar with, but it seems to sometimes be a little overlooked in photography. So in this guide I’ll be explaining color theory to you and looking in detail at the color wheel. Color can be such a useful tool for evoking emotion, so it’s worth understanding how to utilize it.\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\nHere’s what we’ll cover:\\n· The color wheel\\n· Emotions\\n· Color schemes\\n· Color variables\\n· Tips for experimenting\\n· A quick list of helpful tools\\nsa\\nRecommended Reading: Want to create\\nmemorable, fascinating, and impressive color photographs? Grab a copy of Photzy’s premium guide: Rich and Vibrant Color Photography Volume 1.\\n3\\nprimary complementary complementary complementary\\nThe Color Wheel\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\",\n",
       " 'THE COLOR WHEEL\\nKey Lesson: Based on the RYB color system, which is used by painters, this is the most common color wheel. Its primary colors are red, yellow, and blue, and when you mix these together you’ll end up with the secondary colors of orange, green, and violet, combining these results in one of six tertiary colors: red-orange, yellow-orange, yellow- green, blue-green, blue-violet, or red-violet. Of course, as photographers, we’re more used to the RGB (red, green, blue) system used in camera technology and on computers. But when I’m talking about color theory, I’ll be referring to the traditional artistic color wheel.\\n4\\nAlthough obviously not set in stone, different colors tend to evoke different emotions in a viewer. Here are my thoughts on a few of the most common colors:\\n· Blue – cold, trust, sadness, serenity\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\nEMOTIONS\\n· Red – passion, anger, energy\\n· Orange – warmth, happiness, enthusiasm\\n· Green – calm, natural, balance\\n· Yellow – cheerfulness, friendliness\\n5\\nColor Schemes Monochromatic Analogous Complementary OO Split-Complementary Tetradic OO\\nColor Schemes\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\nCOLOR SCHEMES\\nAs I’ve already mentioned, using the color wheel can strengthen your imagery and add interest. Let’s look at the six most commonly used color schemes.\\nMONOCHROMATIC\\nA monochromatic color scheme is one that uses one of the twelve colors on the color wheel with different tints, shades, and tones. In this scenario, tint refers to adding white to a color, shade, adding blacks and tones, or adding gray. Using one color throughout an image with just a combination of gray and black notes can bring a sense of harmony and calm to an image or be used to create a bold and dramatic statement shot. It’s all about using the single color you choose to create a mood that’s appropriate to it.\\n6\\nANALOGOUS\\nAnalogous color schemes use three colors that sit next to each other on the color wheel. It’s a useful technique to create a sense of flow in an image and is a scheme that’s quite often found naturally in nature. Try picking one dominant color and use the other two in a more supporting role. Or you could try the ’60-30-10’ rule often used in design, whereby the main color (which will usually be a primary or secondary color) takes up 60% of the space, the supporting color (secondary or tertiary) 30%, and the final color 10%.',\n",
       " 'COMPLEMENTARY\\nPerhaps the simplest color scheme to use, complementary colors are those on opposite sides of the color wheel (e.g. red and green or blue and orange). When you combine the two, you’ll provide contrast in your images, as well as a pop of color. Complementary colors can be very dramatic and can help make one color look more active.\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\nSPLIT COMPLEMENTARY\\nSplit complementary color schemes are a variation on a complementary color palette. You select your base color, for example blue, but instead of selecting orange opposite, you’ll use the two colors either side of the orange. The colors will still complement each other, but the resulting image should be a little softer than a straight complementary image.\\nTRIAD\\nTriad or triadic color schemes use any three colors that are evenly spaced around the color wheel (for example red, yellow, blue). Using a triadic color palette produces vibrant images that are bursting with contrast. This is because the three colors work in harmony to bring out the tones in the others. In the majority of situations, you’re best off using the three primary (red, yellow, blue) or secondary colors (orange, violet, green). Using too many tertiary colors tends to produce a more ‘muddied’ image that doesn’t have enough contrast and color to ‘pop.’\\n7\\nPhotograph by Güner Deliaga Sahiner\\nA perfect example of the use of two complementary colors producing a beautifully balanced image.\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\nTETRADIC\\nThis is the trickiest of the color schemes to get right, as it uses four colors. You might also hear it referred to as a ‘double complementary’ scheme, as it includes two sets of complementary colors. So you could, for example, use orange and blue alongside green and red. This is probably the scheme that’s easiest to get right in a controlled environment such as a photographic studio or at home where you have control over the colors you include in your image.\\n8',\n",
       " 'COLOR VARIABLES\\nUnderstanding color variables alongside color theory will help to give you images with accurate balance and harmony. There are three basic variables (also called components) of color: hue, value, and saturation.\\nHUE\\nIf we’re talking about color in a colloquial fashion, we’re really talking about hue. Red, yellow, blue, green, etc. are all examples of hues. To simplify things greatly, a color’s hue is determined by light frequency. Red has a lower frequency, blue has a higher one, and green is in the middle. You’ll often find hues measured in degrees using a similar color wheel to the ones we’ve already talked about. By tweaking one hue in an image, you can easily evoke a completely different feel in your image and different emotional responses in your viewers.\\nVALUE\\nA color’s value refers to how light or dark that color is. So white is the lightest, black the darkest, and all other colors fall in between. When you’re working with a single hue (color), it’s comparatively easy to\\nsee the difference between high value and low value colors, and the most successful images tend to have a range of different values in them that are still similar enough to create the balance and depth needed. Of course, when you add in more colors and start using more complicated color schemes, it can become trickier to spot variations in value. I find the easiest way to practice this is to convert your images into black and white. If there’s too much middle gray, you’ll be lacking in tonal contrast.\\nSATURATION\\nAlso known as intensity, saturation measures the purity of a color. But what is color purity? A color that’s heavily saturated has no gray mixed into it. The more gray you add, the more desaturated a color becomes. In painting, a common technique to desaturate color is to mix it with the opposite color on the color wheel. Vibrant and bold colors that are heavily saturated will grab a viewer’s attention and also help an image to ‘leap’ off the page. But of course, this isn’t always the look you want in an image. Desaturated colors produce gentler images and can be used to convey a softer, ethereal, or nostalgic feel to a viewer.\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\n9\\nPhotograph by Pawel Czerwinski\\nPale pink provides a desaturated look that gives this abstract image a gentle and soft feel.\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\n10',\n",
       " 'TIPS FOR EXPERIMENTING\\nThere are many ways to start using color theory and far more than I could cover in one guide. Here are a few tips to get started with:\\n· Look for natural combinations in nature: An easy way to experiment with the color wheel is to look for it in nature. For instance, you could look for complementary colors such as a red rose against green grass or leaves.\\n· Colored lighting: Play around with color that’s under your control by using colored lighting. You can buy colored gels to use over your flashgun (or studio lights), or experiment with colored bulbs in a household light. When out and about, a neon sign on a building or in a shop window can add a splash of color.\\n· Be surreal: Color theory can reward a surreal approach when working with colors that aren’t entirely natural. Infra-red film, for example, creates an unusual color palette, and this can easily be recreated in post-production. Bold, unnatural colors can really emphasize your use of the color wheel.\\n· Use your favorite color: Take your favorite color and use it as a way to challenge yourself when you’re out shooting. Use this color as your primary color and place it in different genres (landscape, urban, portrait, architecture, etc.) whilst employing different color schemes to balance it out. You could even make this color into your signature style.\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\n11\\nA QUICK LIST OF HELPFUL TOOLS\\n· Shoot in RAW: This gives you the most control over your colors in post-production.\\n· Calibrate: Make sure your monitor has been calibrated correctly so that the colors you’re seeing on your screen are a true reflection of how they look in real life. Make sure you edit in a well-lit room as well, so you can see things clearly.\\n· Polarizing Filter: Carry a polarizing filter in your kit for when you’re shooting color. It helps to increase vibrancy in color images and reduces glare, meaning you’ll spend less time tweaking things in post-production.\\nRecommended Reading: Want to create memorable, fascinating, and impressive color photographs? Grab a copy of Photzy’s premium guide: Rich and Vibrant Color Photography Volume 1.\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\n12\\nPhotograph by Sharon Pittaway\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize langchain and chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedding_function = OllamaEmbeddings(model='nomic-embed-text')\n",
    "vectorstore = Chroma(\n",
    "    collection_name='photography_collection', \n",
    "    embedding_function=embedding_function,\n",
    "    persist_directory='../chromadb'\n",
    ")\n",
    "\n",
    "vectorstore.reset_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "docstore = InMemoryStore()\n",
    "id_key = 'doc_id'\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    id_key=id_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "doc_ids = [str(uuid4()) for _ in example_chunks]\n",
    "texts = [\n",
    "    Document(\n",
    "        page_content=text,\n",
    "        metadata={\n",
    "            id_key: doc_ids[i],\n",
    "            'content': text,\n",
    "            'filename': example_chunks[i].metadata.filename,\n",
    "            'page_number': example_chunks[i].metadata.page_number\n",
    "        }\n",
    "    ) for i, text in enumerate(cleaned_text)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, cleaned_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"photzyTM\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU)\\nQuick Guide Written by Jo Plumridge\\nAdvertise with us SPONSORED Before you dive into this guide, here's a few other free resources to help you learn photography: Free Photography eBooks 3 Free Photography Cheat Sheets What is Your #1 Photography Killer? Take this 30 second quiz to find out Free access to our library of 250+ Grab 3 free photography cheat the #1 thing holding your downloadable (pdf) tutorials on sheets that will help you photography back. everything you can imagine. understand the basics. Take Quiz → Download Cheat Sheets → Download eBooks → Want quick photography tips? Check out our friends at DailyPhotoTips.com they'll send you 1 solid photography tip to your inbox, 5 days a week. So you can start your day right, with actionable tips to help you on your creative journey. Subscribe now → (free for a limited time) SPONSORED Advertise with us\\nWhat is Your #1 Photography Killer?\\nTake Quiz →\\nDownload eBooks →\\nColor is everywhere in photography (unless, of course, you’re shooting in black and white), and understanding how to use it correctly is an important step in your photographic journey. Color theory is something that every art student will be familiar with, but it seems to sometimes be a little overlooked in photography. So in this guide I’ll be explaining color theory to you and looking in detail at the color wheel. Color can be such a useful tool for evoking emotion, so it’s worth understanding how to utilize it.\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\nHere’s what we’ll cover:\\n· The color wheel\\n· Emotions\\n· Color schemes\\n· Color variables\\n· Tips for experimenting\\n· A quick list of helpful tools\\nsa\\nRecommended Reading: Want to create\\nmemorable, fascinating, and impressive color photographs? Grab a copy of Photzy’s premium guide: Rich and Vibrant Color Photography Volume 1.\\n3\\nprimary complementary complementary complementary\\nThe Color Wheel\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\",\n",
       " \"CONCLUSION\\nColor theory can add so much to color photography and, although it may look complex, understanding it is actually a logical process. It’s worth taking the time to get to grips with.\\n13\\nSelf-Check Quiz:\\n1) What are the secondary colors?\\n2) What emotion is green associated with?\\n3) What is an analogous color scheme?\\n4) What is saturation also known as?\\n5) Which filter is useful for practicing color theory with?\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\n14\\nAdvertise with us SPONSORED Hey there! Let's get real for a minute... Learning photography can be super challenging! But we're here to help you every step of the way! Here are 3 of our most useful (and FREE!) photography resources: Free Photography eBooks 3 Free Photography Cheat Sheets What is Your #1 Photography Killer? Take this 30 second quiz to find out Free access to our library of 250+ Grab 3 free photography cheat the #1 thing holding your downloadable (pdf) tutorials on sheets that will help you photography back. everything you can imagine. understand the basics. Download eBooks → Take Quiz → Download Cheat Sheets → Want quick photography tips? Check out our friends at DailyPhotoTips.com they'll send you 1 solid photography tip to your inbox, 5 days a week. So you can start your day right, with actionable tips to help you on your creative journey. Subscribe now → (free for a limited time) SPONSORED Advertise with us\\nWhat is Your #1 Photography Killer?\\nDownload eBooks →\\nTake Quiz →\\nDownload Cheat Sheets →\\nABOUT THE AUTHOR\\nJo Plumridge is a UK based freelance writer and photographer. She writes photography, travel, and opinion pieces for magazines, websites, and books, and specializes in portrait and corporate photography. You can view some of her work on her website, www.joplumridge.co.uk, and follow her on Twitter at JoPlumridge.\\nCongratulations! You’ve completed this Photzy guide!\\nIf you liked this photography tutorial, check out this premium guide to help you create memorable, fascinating, and impressive color photographs: Rich and Vibrant Color Photography Volume 1.\\nCREATING RICH & VIBRANT COLOR PHOTOGRAPHY y VOLUME ONE\\nIF YOU’D LIKE TO CONTINUE LEARNING AND IMPROVING YOUR PHOTOGRAPHY PLEASE VISIT PHOTZY.COM\",\n",
       " 'COMPLEMENTARY\\nPerhaps the simplest color scheme to use, complementary colors are those on opposite sides of the color wheel (e.g. red and green or blue and orange). When you combine the two, you’ll provide contrast in your images, as well as a pop of color. Complementary colors can be very dramatic and can help make one color look more active.\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\nSPLIT COMPLEMENTARY\\nSplit complementary color schemes are a variation on a complementary color palette. You select your base color, for example blue, but instead of selecting orange opposite, you’ll use the two colors either side of the orange. The colors will still complement each other, but the resulting image should be a little softer than a straight complementary image.\\nTRIAD\\nTriad or triadic color schemes use any three colors that are evenly spaced around the color wheel (for example red, yellow, blue). Using a triadic color palette produces vibrant images that are bursting with contrast. This is because the three colors work in harmony to bring out the tones in the others. In the majority of situations, you’re best off using the three primary (red, yellow, blue) or secondary colors (orange, violet, green). Using too many tertiary colors tends to produce a more ‘muddied’ image that doesn’t have enough contrast and color to ‘pop.’\\n7\\nPhotograph by Güner Deliaga Sahiner\\nA perfect example of the use of two complementary colors producing a beautifully balanced image.\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\nTETRADIC\\nThis is the trickiest of the color schemes to get right, as it uses four colors. You might also hear it referred to as a ‘double complementary’ scheme, as it includes two sets of complementary colors. So you could, for example, use orange and blue alongside green and red. This is probably the scheme that’s easiest to get right in a controlled environment such as a photographic studio or at home where you have control over the colors you include in your image.\\n8',\n",
       " 'THE COLOR WHEEL\\nKey Lesson: Based on the RYB color system, which is used by painters, this is the most common color wheel. Its primary colors are red, yellow, and blue, and when you mix these together you’ll end up with the secondary colors of orange, green, and violet, combining these results in one of six tertiary colors: red-orange, yellow-orange, yellow- green, blue-green, blue-violet, or red-violet. Of course, as photographers, we’re more used to the RGB (red, green, blue) system used in camera technology and on computers. But when I’m talking about color theory, I’ll be referring to the traditional artistic color wheel.\\n4\\nAlthough obviously not set in stone, different colors tend to evoke different emotions in a viewer. Here are my thoughts on a few of the most common colors:\\n· Blue – cold, trust, sadness, serenity\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\nEMOTIONS\\n· Red – passion, anger, energy\\n· Orange – warmth, happiness, enthusiasm\\n· Green – calm, natural, balance\\n· Yellow – cheerfulness, friendliness\\n5\\nColor Schemes Monochromatic Analogous Complementary OO Split-Complementary Tetradic OO\\nColor Schemes\\nCOLOR THEORY (AND HOW TO MAKE IT WORK FOR YOU) // © PHOTZY.COM\\nCOLOR SCHEMES\\nAs I’ve already mentioned, using the color wheel can strengthen your imagery and add interest. Let’s look at the six most commonly used color schemes.\\nMONOCHROMATIC\\nA monochromatic color scheme is one that uses one of the twelve colors on the color wheel with different tints, shades, and tones. In this scenario, tint refers to adding white to a color, shade, adding blacks and tones, or adding gray. Using one color throughout an image with just a combination of gray and black notes can bring a sense of harmony and calm to an image or be used to create a bold and dramatic statement shot. It’s all about using the single color you choose to create a mood that’s appropriate to it.\\n6\\nANALOGOUS\\nAnalogous color schemes use three colors that sit next to each other on the color wheel. It’s a useful technique to create a sense of flow in an image and is a scheme that’s quite often found naturally in nature. Try picking one dominant color and use the other two in a more supporting role. Or you could try the ’60-30-10’ rule often used in design, whereby the main color (which will usually be a primary or secondary color) takes up 60% of the space, the supporting color (secondary or tertiary) 30%, and the final color 10%.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = retriever.invoke('How do I control my flash during my photography?')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class Question(BaseModel):\n",
    "    prompt: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Question(prompt=['What is color theory?', 'How can the color wheel be used to enhance my photography?', 'Are there any additional resources recommended in this text?'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "model = ChatOllama(model='llama3.1', temperature=.8)\n",
    "question_parser = PydanticOutputParser(pydantic_object=Question)\n",
    "question_prompt = PromptTemplate(\n",
    "    template=\"\"\"Generate 3 different FaQ questions based on the text and only generate questions strictly on the text provided. Do not include any markdown formatting or code blocks in your response.\n",
    "    {format_instructions}\n",
    "    {chunk}\n",
    "\n",
    "    Only return the output in the following format:\n",
    "    {{\n",
    "        \"prompt\": [\"First question here\", \"Second question here\", \"Third question here\"]\n",
    "    }}\n",
    "    \"\"\",\n",
    "    input_variables=['chunk'],\n",
    "    partial_variables={\"format_instructions\": question_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "clean_output = RunnableLambda(\n",
    "lambda x: x.split(\"</think>\")[-1].strip() if isinstance(x, str) \n",
    "else x.content.split(\"</think>\")[-1].strip()\n",
    ")\n",
    "\n",
    "question_chain = question_prompt | model | clean_output | question_parser\n",
    "example_questions = question_chain.invoke(cleaned_text[0])\n",
    "example_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answer candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Answer(prompt=['Color theory is a fundamental concept in art and photography that explains how colors interact to create specific visual effects and emotions.', 'It involves understanding the components of color, such as hue (the actual color), value (the lightness or darkness), and saturation (the intensity or purity of the color).', 'There are various color schemes used in color theory, such as complementary (opposite colors on the color wheel), triadic (three evenly spaced colors), tetradic (two sets of complementary colors), and others, each serving a different aesthetic purpose.'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    prompt: List[str]\n",
    "\n",
    "def parse_docs(docs):\n",
    "    return [doc for doc in docs]\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "    question = kwargs['question']\n",
    "    context = kwargs['context']\n",
    "    \n",
    "    answer_prompt = [\n",
    "        {\n",
    "            'type': 'text',\n",
    "            'text': f\"\"\"Generate 3 different FaQ answers based only on the following context, which only include text. Do not include any markdown formatting or code blocks in your response.\n",
    "            Context: {context}\n",
    "            Question: {question}\n",
    "\n",
    "            Return the output in the following JSON format:\n",
    "            {{\n",
    "                \"prompt\": [\"First answer here\", \"Second answer here\", \"Third answer here\"]\n",
    "            }}\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            HumanMessage(content=answer_prompt)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "answer_chain = (\n",
    "    {\n",
    "        'context': retriever | RunnableLambda(parse_docs),\n",
    "        'question': RunnablePassthrough()\n",
    "    }\n",
    "    | RunnableLambda(build_prompt)\n",
    "    | model\n",
    "    | clean_output\n",
    "    | PydanticOutputParser(pydantic_object=Answer)\n",
    ")\n",
    "\n",
    "example_question = example_questions.prompt[0]\n",
    "candidate_example_question = answer_chain.invoke(example_question)\n",
    "candidate_example_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Color theory is a fundamental concept in art and photography that explains how colors interact to create specific visual effects and emotions.',\n",
       " 'It involves understanding the components of color, such as hue (the actual color), value (the lightness or darkness), and saturation (the intensity or purity of the color).',\n",
       " 'There are various color schemes used in color theory, such as complementary (opposite colors on the color wheel), triadic (three evenly spaced colors), tetradic (two sets of complementary colors), and others, each serving a different aesthetic purpose.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "reference = retriever.vectorstore.similarity_search(example_question, k=1)[0].page_content\n",
    "\n",
    "# score similarity\n",
    "candidate_embeds = embedding_function.embed_documents(candidate_example_question.prompt)\n",
    "reference_embed = embedding_function.embed_query(reference)\n",
    "\n",
    "scores = [\n",
    "    torch.nn.functional.cosine_similarity(\n",
    "        torch.tensor(reference_embed),\n",
    "        torch.tensor(candidate_embed),\n",
    "        dim=0\n",
    "    ).item()\n",
    "    for candidate_embed in candidate_embeds\n",
    "]\n",
    "\n",
    "sorted_pairs = sorted(zip(candidate_example_question.prompt, scores), key=lambda x: x[1], reverse=True)\n",
    "sorted_answer = [pair[0] for pair in sorted_pairs]\n",
    "sorted_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create preference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'What is color theory?',\n",
       " 'chosen': 'Color theory is a fundamental concept in art and photography that explains how colors interact to create specific visual effects and emotions.',\n",
       " 'rejected': 'There are various color schemes used in color theory, such as complementary (opposite colors on the color wheel), triadic (three evenly spaced colors), tetradic (two sets of complementary colors), and others, each serving a different aesthetic purpose.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = {\n",
    "    'prompt': example_question,\n",
    "    'chosen': sorted_answer[0],\n",
    "    'rejected': sorted_answer[-1]\n",
    "}\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "def chunk_file(fname):\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    chunks = partition_pdf(\n",
    "        filename=fname,\n",
    "        strategy=PartitionStrategy.HI_RES,\n",
    "        languages=['eng'],\n",
    "        chunking_strategy='by_title',\n",
    "        max_characters=10000,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        new_after_n_chars=6000,\n",
    "        extract_images_in_pdf=False, # skip image extraction\n",
    "        infer_table_structure=False # skip table extraction\n",
    "    )\n",
    "\n",
    "    return [clean_text(chunk.text) for chunk in chunks]\n",
    "\n",
    "chunk_dict = {}\n",
    "data_dir = os.listdir('../pdf')\n",
    "for i, fname in enumerate(data_dir):\n",
    "    print(f'[{i+1}/{len(data_dir)}] Chunking {fname}')\n",
    "    key = fname.split('.')[0]\n",
    "    path = os.path.join('../pdf', fname)\n",
    "    result = chunk_file(path)\n",
    "    chunk_dict[key] = result\n",
    "    print(f'[{i+1}/{len(data_dir)}] Chunking {fname} - Total chunk: {len(result)}')\n",
    "display.clear_output()\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "vectorstore.reset_collection()\n",
    "\n",
    "def populate_db(fname, chunks):\n",
    "    doc_ids = [str(uuid4()) for _ in chunks]\n",
    "    texts = [\n",
    "        Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                id_key: doc_ids[i],\n",
    "                'content': text,\n",
    "                'filename': fname\n",
    "            }\n",
    "        ) for i, text in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "    retriever.vectorstore.add_documents(texts)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, chunks)))\n",
    "\n",
    "for k, v in chunk_dict.items():\n",
    "    print(f'Popuplating {k}')\n",
    "    populate_db(k, v)\n",
    "display.clear_output()\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CONTINUOUS SHOOTING MODE\\nContinuous mode, sometimes also called burst mode, allows the camera to keep shooting while holding down the shutter release button.\\nContinuous mode is very helpful when photographing fast action, like wildlife, sports, or pets and children playing.\\nThis action tells the camera to take photos one right after another, capturing the images in rapid succession.\\nIt also works well for wedding photographers who need to capture spur-of-the-moment candid photos.\\nSome cameras have two options for continuous shooting – continuous high and continuous low.\\nThe high setting takes the series of photos faster than the low setting.\\nThe maximum speed of the continuous drive modes can range from 3 to 10 frames per second.\\nUsing continuous drive mode gives the photographer a wider range of shots to choose from!\\nHaving a series of shots taken one right after another helps to ensure you don’t miss the peak action.\\nThe image on the following page was taken in a series of six shots while the groom was getting ready to spin his bride. The first two images did not make the final cut, but the third image did!\\nBe sure to check your camera manual to see how many images your camera can take per second in one or both continuous drive modes.\\nI was able to use the continuous drive mode to capture a beautiful and candid moment.\\nCAMERA BASICS SERIES: DRIVE MODES EXPLAINED // © PHOTZY.COM\\n5\\nPhotograph by Angela Fulks\\nAs a wedding photographer, I know that memorable moments happen quickly. I use continuous shooting mode more than any other to capture these moments.\\nCAMERA BASICS SERIES: DRIVE MODES EXPLAINED // © PHOTZY.COM\\n6\\nPhotograph by Angela Fulks\\nThis photo was taken with my camera on a tripod and using the self-timer mode.\\nCAMERA BASICS SERIES: DRIVE MODES EXPLAINED // © PHOTZY.COM\\nSELF-TIMER MODE\\nSelf-timer mode allows a camera to take a photo without the photographer touching the camera at the moment of exposure.\\nMost of us are familiar with the self-timer function on our cell phones; a camera’s self-timer works the same way.\\nMost cameras have a self-timer delay that is anywhere from 2 to 10 seconds after pressing the shutter release button.\\nThis gives the photographer time to jump into a family photo or to experiment with creative self-portraits.\\nThe self-timer mode is also useful for taking long exposure photos. During longer exposures, the slightest shake of the camera can cause a blurry photo. Even the slight movement of pressing the shutter release button can ruin a shot.\\nTo use the self-timer delay to your advantage, step away from your camera after pressing the shutter release button for a sharper photo.\\nIn the photo on the left, I wanted to capture the motion of the fireworks but also keep the scenery sharp to showcase the impressive fireworks display.\\nThe self-timer drive mode, along with lots of patience, allowed me to get a shot that I was happy with.\\n7'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = retriever.invoke('How do I control my flash during my photography?')\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating question for Camera+Basics_Drive+Modes+Explained\n",
      "Generating question for Color+Theory_Make+it+Work\n",
      "Generating question for Composition+Framing+and+Space\n",
      "Generating question for 2023+LR+Spring+Release\n",
      "Generating question for Asymmetrical+Balance\n",
      "Generating question for Creative+Nature\n",
      "Generating question for Crazy+High+Contrast\n",
      "Generating question for Composition+Puzzle\n",
      "Generating question for Bugs_Spiders_Not_Included\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Invalid json output: ```json\n{\n    \"prompt\": [\n        \"What is macro photography?\"\n        \"Which camera settings can be set to Auto to help you get better results?\"\n        \"Is it necessary to travel to specialized locations to shoot insects? Is a noisy image still considered a great image.\"\n    ]\n}\n```\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/output_parsers/json.py:83\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_json_markdown(text)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/utils/json.py:144\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    143\u001b[0m     json_str \u001b[38;5;241m=\u001b[39m json_string \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse_json(json_str, parser\u001b[38;5;241m=\u001b[39mparser)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/utils/json.py:160\u001b[0m, in \u001b[0;36m_parse_json\u001b[0;34m(json_str, parser)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parser(json_str)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/utils/json.py:118\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(s, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39mdecode(s)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 4 column 9 (char 63)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating question for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[0;32m---> 24\u001b[0m         question \u001b[38;5;241m=\u001b[39m generate_question(chunk)\n\u001b[1;32m     25\u001b[0m         question_lists\u001b[38;5;241m.\u001b[39mappend(question)\n\u001b[1;32m     26\u001b[0m display\u001b[38;5;241m.\u001b[39mclear_output()\n",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m, in \u001b[0;36mgenerate_question\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m      4\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGenerate 3 different FaQ question based on the text and only generate question strictly on the text provided. Do not include any markdown formatting or code blocks in your response.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{format_instructions}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     partial_variables\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_instructions\u001b[39m\u001b[38;5;124m\"\u001b[39m: parser\u001b[38;5;241m.\u001b[39mget_format_instructions()}\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m model \u001b[38;5;241m|\u001b[39m clean_output \u001b[38;5;241m|\u001b[39m parser\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chain\u001b[38;5;241m.\u001b[39minvoke(text)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3021\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:202\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    195\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    205\u001b[0m         config,\n\u001b[1;32m    206\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    207\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/runnables/base.py:1925\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1922\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1923\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1924\u001b[0m         Output,\n\u001b[0;32m-> 1925\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m   1926\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1927\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1928\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1929\u001b[0m             config,\n\u001b[1;32m   1930\u001b[0m             run_manager,\n\u001b[1;32m   1931\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1932\u001b[0m         ),\n\u001b[1;32m   1933\u001b[0m     )\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1935\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:203\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    195\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    205\u001b[0m         config,\n\u001b[1;32m    206\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    207\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/output_parsers/pydantic.py:72\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partial:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/output_parsers/pydantic.py:67\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse the result of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    The parsed pydantic object.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mparse_result(result)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_obj(json_object)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/output_parsers/json.py:86\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     85\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid json output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg, llm_output\u001b[38;5;241m=\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Invalid json output: ```json\n{\n    \"prompt\": [\n        \"What is macro photography?\"\n        \"Which camera settings can be set to Auto to help you get better results?\"\n        \"Is it necessary to travel to specialized locations to shoot insects? Is a noisy image still considered a great image.\"\n    ]\n}\n```\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "def generate_question(text):\n",
    "    parser = PydanticOutputParser(pydantic_object=Question)\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Generate 3 different FaQ question based on the text and only generate question strictly on the text provided. Do not include any markdown formatting or code blocks in your response.\n",
    "        {format_instructions}\n",
    "        {chunk}\n",
    "\n",
    "        Return the output in the following format:\n",
    "        {{\n",
    "            \"prompt\": [\"First question here\", \"Second question here\", \"Third question here\"]\n",
    "        }}\n",
    "        \"\"\",\n",
    "        input_variables=['chunk'],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    chain = prompt | model | clean_output | parser\n",
    "    return chain.invoke(text)\n",
    "\n",
    "question_lists = []\n",
    "for k, chunks in chunk_dict.items():\n",
    "    print(f'Generating question for {k}')\n",
    "    for chunk in chunks:\n",
    "        question = generate_question(chunk)\n",
    "        question_lists.append(question)\n",
    "display.clear_output()\n",
    "question_lists[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m question_lists:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mprompt:\n\u001b[0;32m---> 45\u001b[0m         qna_pairs[question] \u001b[38;5;241m=\u001b[39m generate_answer(question)\n",
      "Cell \u001b[0;32mIn[36], line 40\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m     25\u001b[0m         [\n\u001b[1;32m     26\u001b[0m             HumanMessage(content\u001b[38;5;241m=\u001b[39manswer_prompt)\n\u001b[1;32m     27\u001b[0m         ]\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     30\u001b[0m chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     31\u001b[0m     {\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: retriever \u001b[38;5;241m|\u001b[39m RunnableLambda(parse_docs),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;241m|\u001b[39m PydanticOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39mAnswer)\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chain\u001b[38;5;241m.\u001b[39minvoke(text)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3021\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    634\u001b[0m                 m,\n\u001b[1;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    638\u001b[0m             )\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    852\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    853\u001b[0m         )\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/langchain_anthropic/chat_models.py:797\u001b[0m, in \u001b[0;36mChatAnthropic._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[1;32m    796\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_payload(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 797\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/anthropic/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/anthropic/resources/messages/messages.py:901\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[1;32m    895\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    896\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    898\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    899\u001b[0m     )\n\u001b[0;32m--> 901\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/v1/messages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    903\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    904\u001b[0m         {\n\u001b[1;32m    905\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    906\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    907\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    908\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    909\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop_sequences,\n\u001b[1;32m    910\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    911\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m: system,\n\u001b[1;32m    912\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    914\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    915\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_k,\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    917\u001b[0m         },\n\u001b[1;32m    918\u001b[0m         message_create_params\u001b[38;5;241m.\u001b[39mMessageCreateParams,\n\u001b[1;32m    919\u001b[0m     ),\n\u001b[1;32m    920\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    921\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    922\u001b[0m     ),\n\u001b[1;32m    923\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mMessage,\n\u001b[1;32m    924\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    925\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mStream[RawMessageStreamEvent],\n\u001b[1;32m    926\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/anthropic/_base_client.py:1279\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1267\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1275\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1276\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1277\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1278\u001b[0m     )\n\u001b[0;32m-> 1279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/anthropic/_base_client.py:956\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    957\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    958\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    959\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    960\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    961\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m    962\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sandbox/lib/python3.11/site-packages/anthropic/_base_client.py:1060\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1059\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1063\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1064\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1069\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
     ]
    }
   ],
   "source": [
    "def generate_answer(text):\n",
    "    def parse_docs(docs):\n",
    "        return [doc for doc in docs]\n",
    "\n",
    "    def build_prompt(kwargs):\n",
    "        question = kwargs['question']\n",
    "        context = kwargs['context']\n",
    "        \n",
    "        answer_prompt = [\n",
    "            {\n",
    "                'type': 'text',\n",
    "                'text': f\"\"\"Generate 3 different FaQ answers based only on the following context, which only include text. Do not include any markdown formatting or code blocks in your response.\n",
    "                Context: {context}\n",
    "                Question: {question}\n",
    "\n",
    "                Return the output in the following JSON format:\n",
    "                {{\n",
    "                    \"prompt\": [\"First answer here\", \"Second answer here\", \"Third answer here\"]\n",
    "                }}\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        return ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                HumanMessage(content=answer_prompt)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            'context': retriever | RunnableLambda(parse_docs),\n",
    "            'question': RunnablePassthrough()\n",
    "        }\n",
    "        | RunnableLambda(build_prompt)\n",
    "        | model\n",
    "        | clean_output\n",
    "        | PydanticOutputParser(pydantic_object=Answer)\n",
    "    )\n",
    "\n",
    "    return chain.invoke(text)\n",
    "\n",
    "qna_pairs = {}\n",
    "for chunk in question_lists:\n",
    "    print(f'Generating answer for {chunk.prompt}')\n",
    "    for question in chunk.prompt:\n",
    "        qna_pairs[question] = generate_answer(question)\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
